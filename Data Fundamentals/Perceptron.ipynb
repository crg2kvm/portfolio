{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTZW4uoxCVeI"
   },
   "source": [
    "# Homework 02: Linear Classification\n",
    "\n",
    "Deadline: Monday March 13, 2023 11:59 PM (without a late penalty)\n",
    "\n",
    "---\n",
    "\n",
    "This homework consists two sections, which covers the Perceptron algorithm and logistic regression classifiers that we learned. \n",
    "\n",
    "If you are running this notebook on your local computer, please make sure it has the following packages installed\n",
    "\n",
    "- Numpy\n",
    "- Sklearn\n",
    "\n",
    "---\n",
    "\n",
    "**Requirements**:\n",
    "\n",
    "Please read the following requirements carefully, to avoid any unnecessary point deduction. \n",
    "\n",
    "1. Keep all the required results in your submission, our TAs will grade the homework based on the submitted results. (The TAs not run the code unless they have some questions about the implementation.)\n",
    "2. Your submission should only be via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CizxMwf-TTtC"
   },
   "source": [
    "## 1 The Perceptron Algorithm (5 points)\n",
    "\n",
    "### 1.1 Implementation (3.5 points)\n",
    "\n",
    "Let's implement the Perceptron algorithm described in our class lecture. \n",
    "\n",
    "First, we need to download a toy dataset from the course webpage and load it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "eLBWI8RrT8pN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11  0.59  1.  ]\n",
      " [ 0.35  0.79  1.  ]\n",
      " [ 0.25  0.46  1.  ]\n",
      " [ 0.03  0.26  1.  ]\n",
      " [-0.37  0.38  1.  ]\n",
      " [ 0.25  0.12  1.  ]\n",
      " [-0.06 -0.03  1.  ]\n",
      " [ 0.2  -0.2   1.  ]\n",
      " [ 0.55  0.08  1.  ]\n",
      " [ 0.81 -0.11  1.  ]\n",
      " [ 0.54 -0.23  1.  ]\n",
      " [-0.11 -0.23  1.  ]\n",
      " [ 0.14  1.03  1.  ]\n",
      " [ 0.59  1.21  1.  ]]\n",
      "[-1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "url = \"https://yangfengji.net/uva-ml-undergrad/data/separable.txt\"\n",
    "filename, headers = urllib.request.urlretrieve(url, filename=\"separable.txt\")\n",
    "\n",
    "data = arr = np.loadtxt(\"separable.txt\", delimiter=\"\\t\", dtype=float)\n",
    "X = data[:,:2] # Input\n",
    "Y = data[:,-1] # Label\n",
    "\n",
    "# Attach one column to X for capturing the bias term in classification\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj50cVGkWCNC"
   },
   "source": [
    "Please implement the Perceptron algorithm in the following code block.\n",
    "\n",
    "Make sure your implementation has the following components\n",
    "\n",
    "- The Percpetron updating rule and the condition of when to run the updating rule;\n",
    "- The convergence criterion -- stop training when the classifier can make correct prediction of all the training examples;\n",
    "- Print out the classification weight in the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.22 -4.17  1.  ]\n"
     ]
    }
   ],
   "source": [
    "def predict(X, weights,Y):\n",
    "    Output = np.dot(X,weights) * Y\n",
    "    return Output\n",
    "\n",
    "def updateWeights(X,Y,weights):\n",
    "    weights += X*Y\n",
    "    return weights\n",
    "\n",
    "\n",
    "def perceptron(X,Y):\n",
    "    weights = np.zeros(3)\n",
    "    error = True\n",
    "    while error is True:\n",
    "        error = False\n",
    "        for i in range(len(Y)):\n",
    "            guess = predict(X[i],weights,Y[i])\n",
    "            if guess <= 0:\n",
    "                error = True\n",
    "                weights = updateWeights(X[i],Y[i],weights)\n",
    "        if error is False:\n",
    "            break\n",
    "    return weights\n",
    "                \n",
    "            \n",
    "\n",
    "print(perceptron(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4GS1OePMtez"
   },
   "source": [
    "### 1.2 The Difference between Perceptron and Logisitic Regression (1.5 points)\n",
    "\n",
    "Please list the major difference between the Perceptron model and the Logistic Regression model as we discussed in class. Please list at least three of them.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EZm5mHnXs7b"
   },
   "source": [
    "1. The perceptrons y can only be -1 or 1 while the logisitic regression model's y is continuous on [-1,1].\n",
    "2. For logistic regression the weights are updated every prediction and for perceptron the weights are only updated if the prediction is wrong\n",
    "3. Perceptron uses SGD while Logistic can use a wide range of algorithms to update its weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_j2VR5DD6lE"
   },
   "source": [
    "## 2 Logistic Regression (9 points)\n",
    "\n",
    "Logistic regression can handle both linear separable and unseparable cases. Therefore, we will use a real-world example datasets, instead of a synthetical one. \n",
    "\n",
    "We will use the [Logistic Regression]() model from Sklearn for the questions. In other words, you don't have to implement a logistic regression model by yourself, but you need to understand how to use it and how to interpret its outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b8yaLJ_EpK2"
   },
   "source": [
    "### 2.0 Download the dataset\n",
    "\n",
    "Run the following code and download the [xxx]() dataset from [OpenML](). Similar to what we explained in class, we will also use the [OridinalEncoder]() to convert the non-numeric features into numeric features. \n",
    "\n",
    "Furthermore, we will run the data split function to divide the whole dataset into a training set and a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxfQSr6dFECO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 768 examples with 8 features\n",
      "The size of training set: 537\n",
      "The size of validation set: 231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, Y = fetch_openml(\"diabetes\", version=1, return_X_y=True)\n",
    "print(\"Read {} examples with {} features\".format(X.shape[0], X.shape[1]))\n",
    "\n",
    "#print(X)\n",
    "\n",
    "trnX, valX, trnY, valY = train_test_split(X, Y, test_size=0.3, random_state=111)\n",
    "print(\"The size of training set: {}\".format(trnX.shape[0]))\n",
    "print(\"The size of validation set: {}\".format(valX.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGJ5zqnhFEiH"
   },
   "source": [
    "### 2.1 Build the Classifier (2 points)\n",
    "\n",
    "Please follow the instructions and complete the code block for building an LR classifier. \n",
    "\n",
    "- Use the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from Sklearn with its **default** parameters.\n",
    "- Training the model and report its prediction accuracy on **both** the training and the validation sets.\n",
    "- Keep the **printed** (instead of hand typing) results in your submission for grading. Make sure the printed results have enough information about which is training accuracy and which one is validation accuracy. For example, you can use \n",
    "```python\n",
    "print(\"Training accuracy: {}\".format(trn_acc))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwsmDKwFFmyQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7839851024208566\n",
      "Validation accuracy: 0.7575757575757576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camda\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# TODO: implementation\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression()\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "trainpred = classifier.predict(trnX)\n",
    "valpred = classifier.predict(valX)\n",
    "\n",
    "\n",
    "trnY1 = trnY.to_list()\n",
    "traincount = 0\n",
    "for i in range(len(trainpred)):\n",
    "    if trainpred[i] == trnY1[i]:\n",
    "        traincount += 1\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "valY1 = valY.to_list()\n",
    "for i in range(len(valpred)):\n",
    "    if valpred[i] == valY1[i]:\n",
    "        valcount += 1\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {}\".format(traincount/537))\n",
    "\n",
    "print(\"Validation accuracy: {}\".format(valcount/231))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQCfRmwmFnqP"
   },
   "source": [
    "### 2.2 Feature Pre-processing (2 points)\n",
    "\n",
    "In our lecture, we talk about three different ways to represent the features before feeding into the classifier. \n",
    "\n",
    "Pick the [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) to further processing the features, and building the classifier again. \n",
    "\n",
    "Please follow the instructions and complete the code block for building an LR classifier with feature pre-processing\n",
    "\n",
    "- Pick one pre-processing method to further scale or encode the features\n",
    "- Training the model and report its prediction accuracy on **both** the training and the validation sets.\n",
    "- Keep the **printed** (instead of hand typing) results in your submission for grading. Make sure the printed results have enough information about which is training accuracy and which one is validation accuracy.\n",
    "\n",
    "Note that, you can find many other pre-processing functions in Sklearn via [this link](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). Since we only discussed the three abovementioned pre-processing methods in class, you are *not* required to do anything beyond the instruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "aWDbmNt5IiXr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9739292364990689\n",
      "Validation accuracy: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# TODO: implementation\n",
    "\n",
    "X, Y = fetch_openml(\"diabetes\", version=1, return_X_y=True)\n",
    "enc = sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "#enc1 = sklearn.preprocessing.OneHotEncoder()\n",
    "#Y = enc.fit_transform(Y)\n",
    "trnX, valX, trnY, valY = train_test_split(X, Y, test_size=0.3, random_state=111)\n",
    "trnx = enc.fit(trnX)\n",
    "trnX = enc.transform(trnX)\n",
    "#valX = enc.fit_transform(valX)\n",
    "valX = enc.transform(valX)\n",
    "classifier = sklearn.linear_model.LogisticRegression()\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "trainpred = classifier.predict(trnX)\n",
    "valpred = classifier.predict(valX)\n",
    "\n",
    "\n",
    "trnY1 = trnY.to_list()\n",
    "traincount = 0\n",
    "for i in range(len(trainpred)):\n",
    "    if trainpred[i] == trnY1[i]:\n",
    "        traincount += 1\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "valY1 = valY.to_list()\n",
    "for i in range(len(valpred)):\n",
    "    if valpred[i] == valY1[i]:\n",
    "        valcount += 1\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {}\".format(traincount/537))\n",
    "\n",
    "print(\"Validation accuracy: {}\".format(valcount/231))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3nrHVI2IjPB"
   },
   "source": [
    "### 2.3 Justification of Pre-processing (2 points)\n",
    "\n",
    "If your implementation is correct, you should see a significant boost on both training and validation accuracies. \n",
    "\n",
    "Based on what we discussed in class, please explain why this one-hot encoding is helpful for improving classification performance.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JySvBz_4Y-s-"
   },
   "source": [
    " Most machine learning model inputs require that the dimesnions are numerical. one hot encoding transforms these categorigal dimensions into numerical dimensions which allow the model t take them in as input. In addition it removes the ordinality of these categorical variablesmeaning that each category in the categorical dimension is an independent dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8Zlz48wJyOS"
   },
   "source": [
    "### 2.4 $\\ell_2$ Regularization (1.5 points)\n",
    "\n",
    "With the pre-processed data, please train the classifiers with three different regularization coefficient. In the [Logistic Regression]() model implemented in Sklearn, the associated function argument is $C$. \n",
    "\n",
    "Please use the following three code blocks and try three different $C$ values\n",
    "\n",
    "- $C=100.0$\n",
    "- $C=1.0$\n",
    "- $C=0.01$\n",
    "\n",
    "and pirnt out \n",
    "\n",
    "- the training accuracy, and \n",
    "- the validation accuracy \n",
    "\n",
    "by following the **same** format as the previous questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnvXilP4K3W4"
   },
   "source": [
    "$C=100.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "oap5h_QeKFej"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.7229437229437229\n"
     ]
    }
   ],
   "source": [
    "# TODO: LR with C=100.0\n",
    "classifier = sklearn.linear_model.LogisticRegression(C = 100)\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "trainpred = classifier.predict(trnX)\n",
    "valpred = classifier.predict(valX)\n",
    "\n",
    "\n",
    "trnY1 = trnY.to_list()\n",
    "traincount = 0\n",
    "for i in range(len(trainpred)):\n",
    "    if trainpred[i] == trnY1[i]:\n",
    "        traincount += 1\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "valY1 = valY.to_list()\n",
    "for i in range(len(valpred)):\n",
    "    if valpred[i] == valY1[i]:\n",
    "        valcount += 1\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {}\".format(traincount/537))\n",
    "\n",
    "print(\"Validation accuracy: {}\".format(valcount/231))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZboa3xPK6vI"
   },
   "source": [
    "$C=1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "xwAjR11sK7-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7746741154562383\n",
      "Validation accuracy: 0.7012987012987013\n"
     ]
    }
   ],
   "source": [
    "# TODO: LR with C=0.1\n",
    "classifier = sklearn.linear_model.LogisticRegression(C = 0.1)\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "trainpred = classifier.predict(trnX)\n",
    "valpred = classifier.predict(valX)\n",
    "\n",
    "\n",
    "trnY1 = trnY.to_list()\n",
    "traincount = 0\n",
    "for i in range(len(trainpred)):\n",
    "    if trainpred[i] == trnY1[i]:\n",
    "        traincount += 1\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "valY1 = valY.to_list()\n",
    "for i in range(len(valpred)):\n",
    "    if valpred[i] == valY1[i]:\n",
    "        valcount += 1\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {}\".format(traincount/537))\n",
    "\n",
    "print(\"Validation accuracy: {}\".format(valcount/231))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TndTQrHKLHFw"
   },
   "source": [
    "$C=0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "Z9smqQS8LGZY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.6443202979515829\n",
      "Validation accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# TODO: C=0.01\n",
    "\n",
    "classifier = sklearn.linear_model.LogisticRegression(C = 0.01)\n",
    "classifier.fit(trnX,trnY)\n",
    "\n",
    "trainpred = classifier.predict(trnX)\n",
    "valpred = classifier.predict(valX)\n",
    "\n",
    "\n",
    "trnY1 = trnY.to_list()\n",
    "traincount = 0\n",
    "for i in range(len(trainpred)):\n",
    "    if trainpred[i] == trnY1[i]:\n",
    "        traincount += 1\n",
    "\n",
    "valcount = 0\n",
    "\n",
    "valY1 = valY.to_list()\n",
    "for i in range(len(valpred)):\n",
    "    if valpred[i] == valY1[i]:\n",
    "        valcount += 1\n",
    "\n",
    "\n",
    "print(\"Training accuracy: {}\".format(traincount/537))\n",
    "\n",
    "print(\"Validation accuracy: {}\".format(valcount/231))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjrDQry6LRFp"
   },
   "source": [
    "### 2.5 Explan the regularization effect (1.5 points)\n",
    "\n",
    "Based on the classification accuracies above, explain the $\\ell_2$ regularization effect for the classification performance\n",
    "\n",
    "- What is the direct effect of $\\ell_2$ regularization?\n",
    "- Does it work all the time or when it may work?\n",
    "- How do we know it works for helping avoid over-fitting?\n",
    "\n",
    "Make sure your answer covers all the three questions above.\n",
    "\n",
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNXQ5LZCNCJ_"
   },
   "source": [
    "1. The direct effect of l2 is that it reduces the magnitude of the weights in the logistic regression model. Which in turn helps reduce the chances of overfitting.\n",
    "\n",
    "2. l2 does not always work when the features are highly correlated which could lead to unstabl weight estimates.\n",
    "\n",
    "3. When c is a large value and there is a large discrepancy in the training and validation accuracies this could mean there is overfitting because at the large c value the model doesn't penalize large weight."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
